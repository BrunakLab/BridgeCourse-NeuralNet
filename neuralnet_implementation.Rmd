---
output:
  html_document:
    df_print: paged
  output: default
title: "Implementing a Neural Network"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## What is a Neural network (NN)
A neural netowork (NN) can be defined as a model of reasoning based on the human brain. The electric signal in our brain is propagated by neurons. Neurons have the ability to create connections between each others and to adapt to stimulation pattern. This plasticity is the basic mechanism for learning.

Neural netoworks like human brain have some common aspects:

* The knowledge is acquired through a learnign process.
* The knowledge is stored in weights (synaptic strenght in the human brain and numeric contribution in NN)
 
## The neuron

The three basic structures in a NN are the neurons, the architecture and the learning alghoritm.

Neurons are the processing elements: they process the signal received as input and pass it through an activation function, hence generating the output.

<p align="center">![Artificial Neuron from Warren McCulloch and Walter Pitts](https://miro.medium.com/max/645/1*-JtN9TWuoZMz7z9QKbT85A.png)<p>

## The architecture

The architecture defines the number of neurons and their connectivity. A NN usually consists of layers of neurons stacked together; each neuron learns a specific rappresentation of the output from the previous layer based on the weights and the activation function.

---

<br /><p align="center">![Architecture of a simple Feed Forward neural network](https://n2value.com/blog/wp-content/uploads/2016/02/Rplot.jpeg){width=550px}<p><br />

### Feed forward VS Recurrent Networks
 
Since the computation of the final output propagates only in one direction, from the input layer to the output of the model, this kind of networks are called Feed Forward Neural Network (FFNN). The alternative to the FFNN are the recurrent neural network (RNN), here the input of a neuron is the outcome of the same neuron at the previous step. This RNN based architectures are well suited for learning temporal patterns.

## The learnign alghoritm

The learning alghoritm is the mean for the model to store the knowledge in numerical weights. A cost function, usually called loss function, is used for assessing the quality of the prediction. Then the weights of the model are updated in order to minimize the error. This step is called backpropagation, because it propagates in the opposite direction of the feed forward pass. 

### Optimization alghoritm

THe decision of how to modify the weights is up to the optimization algorithm. A common optimization algorithm is the Gradient Descent. 

<br /><p align="center">![gradient descent optimization](http://neuralnetworksanddeeplearning.com/images/valley_with_ball.png){width=450px}<p><br />

## Where to start? 

A nice place where to start with neural networks is the [Tensorflow Playground](https://playground.tensorflow.org/), an interactive visualization web application
written in JavaScript that allows to simulate real time simple neural networks. Tensorflow is a software library often used for machine learing applications (as others like pytorch etc.). On the playground website, you can have a graphical explanation of how the achitecture of a network changes accordingly to the number of neurons and layers, as well as how the activation function and the prediction task affect the learning process. Indeed, finding the right hyperparameters usually requires some time during the model development.

<br /><p align="center">![tensorflow playground](https://miro.medium.com/max/3200/0*Uw7cusbCha9kQOGN.)<p><br />

# Neural network for longitudinal data
The aim of this notebook is to predict 5 years mortality for patients that got diagnosed with hypertension before the age of 65, depending on their disease hystory. In order to accomplish this task, the trajectory extracted previously in the course will be used as input.

Here we load the libraries used for the data transformation, for the neural network implementation and for the results analysis. If any of the library is missing, you can install it using the command *install.packages(pkgs)*.

```{r, echo=TRUE, message=FALSE, results='hide'}
library(tidyverse)
library(lubridate)
library(neuralnet)
library(pROC)
```

Here you can load the three files used in this notebook.
```{r}
cpr <- read.csv('cpr.tsv', sep='\t', header = TRUE)
lpr <- read.csv('lpr.tsv', sep='\t', header = TRUE)
trj <- read.csv('trj.tsv', sep='\t', header = TRUE)

head (cpr)
head (lpr)
head (trj)
```

In order to make transformation to the dates, we need to parse them as such. The command *as.Date* is used to convert the columns in the desired format.
```{r}
cpr$BIRTHDAY <- as.Date(cpr$BIRTHDAY)
cpr$STATUS_DATE <- as.Date(cpr$STATUS_DATE)
lpr$ADM_DATE <- as.Date(lpr$ADM_DATE)
```

Here we select the patients that got hypertension disease code before 65y. In order to do so, we first merge lpr table with cpr table. The function *inner_join* from *dplr* library allows us to merge the two tables. The key for merging is passed to the function through the parameter *by=*. 

```{r}
cpr_to_lpr <- inner_join(cpr, lpr, by='ID') 
```

Then we add the columns *AGE_AT_DIAG* and *AGE_AT_STATUS* using the mutate function in *dplyr.* The two columns indicate how old was the patient when he got first time diagnosed with hypertension and how old he was at the *STATUS_DATE*.

```{r}
cpr_to_lpr <- cpr_to_lpr %>% 
  mutate(
    AGE_AT_DIAG = time_length(BIRTHDAY %--% ADM_DATE, "years") %>% round(1),
    AGE_AT_STATUS = time_length(BIRTHDAY %--% STATUS_DATE, "years") %>% round(1)
  )

head(cpr_to_lpr)
```

At this point, we can filter out the patients that we do not want to include in the study. In this case, as arbitrary choice, we select only the patients who got diagnosed before 65. In addition to this, it is necessary to deal with the censored patients, those patients whose AGE_AT_STATUS is not far enough from the AGE_AT_DIAG, so we do not know if the event of interest, death, has occured between the end of the study period and the fifth year after the year of diagnose. As a simplified example, we can just exclude the censored patient. 

```{r}
cpr_to_lpr <- cpr_to_lpr %>% 
  filter(AGE_AT_DIAG < 65) %>%
  filter((AGE_AT_STATUS - AGE_AT_DIAG) > 5 | STATUS == 90)

head(cpr_to_lpr)
```

The final step is to generate the outcome column. The binary outcome encodes death as 1 and survival as 0.

```{r}

cpr_to_lpr <- cpr_to_lpr %>% 
  mutate (
    OUTCOME  = (AGE_AT_STATUS - AGE_AT_DIAG) < 5 & STATUS == 90 %>% as.integer() #Death needs to occur within 5 years, and LPR STATUS has to be 90. 
  )

head(cpr_to_lpr)
#cpr_to_lpr$OUTCOME <- as.integer((cpr_to_lpr$AGE_AT_STATUS - cpr_to_lpr$AGE_AT_DIAG) < 5 & cpr_to_lpr$STATUS == 90)

```

#### Generate the input
The package neuralnet takes as input a table containing the target (the OUTCOME column generated above) and the input data. In order to generate the input for the model, we first need to extract the unique trajectories all the patients have, so that we can index them.

```{r}
unique_trj <- unique(trj[-1]) # with -1 we exclude the ID column (we would have otherwise a unique row for each patient)

unique_trj$TRJ_IDX <- paste("T", c(1:nrow(unique_trj)), sep='') # here we generate an index for each trajectory. 

head(unique_trj)
```

Now we would like to add this trajectory index, contained in the table *unique_trj*, to the table loaded from *trj.tsv*. We accomplish this using again the function *inner_join*. Now the merging keys are the four diseases constituting each trajectory. 

```{r}
merged_trj <- inner_join(trj, unique_trj, 
                          by = c("Disease1", "Disease2", "Disease3", "Disease4")) %>% 
  select("ID","TRJ_IDX") # Here we select only the column ID and TRJ_IDX, as the single disease information is not used in the model. 

head(merged_trj)
```

Since the trajectory index is a categorical feature, we need to find a way to include this information in the input data. The choice here is to one hot encode it. Using one hot encoding, each row of the table contains a column for each unique trajectory, and a binary value encodes the presence/absence of that specific trajectory for the patient in that row.

```{r}
input_trj <- merged_trj %>% 
  unique() %>%
  mutate(n = 1)%>% 
  spread(TRJ_IDX, n, fill = 0) %>% 
  filter(ID %in% cpr_to_lpr$ID)

head(input_trj)
```

Here we simply add the outcome column merging the *input_trj* file generated above and the *cpr_to_lpr* table.

```{r}
nn_input <- inner_join(input_trj, cpr_to_lpr %>% select(ID, OUTCOME), by="ID") # we select only ID and OUTCOME from cpr_to_lpr
```

When evaluating the perfomance of the model, we use an external dataset that has not been seen by the model during training. This set is called for this reason test set. In the following section we extract 70% of the IDs randomly, and then we use them for extracting the training set. The remaining IDs will populate the test set. 

```{r}
train_id <- nn_input %>%
  sample_frac(0.70) %>% 
  select(ID)

nn_input_train <- nn_input %>% 
  filter(ID %in% train_id$ID) %>% 
  select(- ID)   # We exclude the ID column as we do not want it included in the input data
nn_input_test <- nn_input %>% 
  filter(!ID %in% train_id$ID) %>% 
  select(- ID)  # We exclude the ID column as we do not want it included in the input data
  
```

After playing a bit with the [Tensorflow Playground](https://playground.tensorflow.org/), you have probably noticed that the number of hyperparamters to tune is high, resulting in combinations that can lead to different performances. Even though the parameters of the function *neuralnet* are not exactly the same you found in the playground, the concepts are similar. I would suggest you to have a look at *?neuralnet::neuralnet* documentation to have a full overview of the function. You can try tune the parameters trying to achieve the best AUC value (since you are running the code on your local machine, use restrained number for *stepmax*, *rep*, *hidden*). If you do not know where to start, I would suggest you to change the value of *hidden* (you can also add more hidden layers with c(n1, n2, n3)).

```{r}
nn <-  neuralnet::neuralnet(OUTCOME ~ ., data=nn_input_train, hidden=0, act.fct = "logistic", linear.output = FALSE, stepmax = 1e+04)
```

The plot function allows you to have a visual representation of the network achitecture, including weights and biases. Of course, networks with a lot of neurons and layers will be difficult to plot in a comprehensive way. 

```{r}
plot(nn, rep = "best") #use best for show plot in knit render
```

### Evaluation

Here we evaluate the network on the test set. First we use the compute function for generating the output (the compute function does not update the weights of the network, so the model does not train on the data passed). After that we can use the functions inside *pROC* to extract the metrics we need for the evaluation.

```{r}
pred <- compute(nn, nn_input_test)
roc_obj <- roc(nn_input_test$OUTCOME, pred$net.result)
auc(roc_obj)
coords(roc_obj, c(0.1,0.2, 0.4, 0.5, 0.7), ret=c("tp", "tn", "fp", "fn", "sensitivity", "specificity"))
```

### Additional
Think about other possible ways to encode the disease trajectories as input for the neural network model.

If you run the network without hidden layer, you can have an idea of the impact each trajectory has on the final prediction prediction. Higher is the weight higher is the contribute to pull the prediction towards one.

```{r}
name_weights <- colnames(nn$covariate) #here you take from covariate the name of the input feature in the order the weight matrix is stored
w <- nn$weights[[1]][[1]] #extract the weight matrix
weights_df <- data.frame(c("Bias", name_weights), w) # create a dataframe 
colnames(weights_df) <- c("trajectory", "weight")
ggplot(weights_df, aes(trajectory, weight)) + geom_col(stat='identity') + theme(axis.text.y = element_text(face="bold", color="black",  size=8)) #plot the weight
 
```